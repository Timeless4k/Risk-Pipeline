# GPU-Accelerated SHAP Analysis Guide

This guide explains how to use GPU acceleration for SHAP image generation in the RiskPipeline, which can provide significant speedups for large datasets.

## üöÄ Performance Benefits

GPU-accelerated SHAP can provide:
- **Up to 19x faster** SHAP value computation
- **Up to 340x faster** SHAP interaction values
- **20x faster** than 40-core CPU for moderate-sized models
- **Memory-efficient** batch processing for large datasets

## üìã Prerequisites

### 1. CUDA Installation
Ensure you have CUDA installed on your system:
```bash
# Check CUDA version
nvcc --version

# Check PyTorch CUDA support
python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')"
```

### 2. Install GPU-Accelerated SHAP
```bash
# Run the installation script
python install_gpu_shap.py

# Or manually install
pip install shap[gpu] --upgrade
```

## ‚öôÔ∏è Configuration

### Enable GPU Acceleration
In your pipeline configuration:

```python
from risk_pipeline.core.config import PipelineConfig

config = PipelineConfig(
    # ... other settings ...
    shap=PipelineConfig.SHAPConfig(
        use_gpu=True,                    # Enable GPU acceleration
        gpu_memory_fraction=0.8,        # Use 80% of GPU memory
        batch_size_gpu=1000,            # Batch size for GPU processing
        background_samples=500,          # Background samples for SHAP
        max_display=20,                 # Max features to display
        plot_type='bar'                 # Plot types to generate
    )
)
```

### Memory Management
The GPU SHAP processor automatically manages memory:
- Clears GPU cache before/after computations
- Uses configurable memory fraction
- Processes large datasets in batches
- Falls back to CPU if GPU memory is insufficient

## üîß Usage Examples

### Basic GPU SHAP Analysis
```python
from risk_pipeline import RiskPipeline
from risk_pipeline.core.config import PipelineConfig

# Configure for GPU acceleration
config = PipelineConfig(
    use_gpu_shap=True,
    gpu_memory_fraction=0.8
)

# Initialize pipeline
pipeline = RiskPipeline(config)

# Run with GPU-accelerated SHAP
results = pipeline.run_pipeline(
    assets=['AAPL', 'MSFT'],
    models=['xgboost', 'lstm', 'stockmixer'],
    tasks=['regression', 'classification']
)
```

### Manual GPU SHAP Processing
```python
from risk_pipeline.utils.gpu_shap_utils import get_gpu_shap_processor

# Get GPU processor
gpu_processor = get_gpu_shap_processor(config)

# Process SHAP values in batches
shap_values = gpu_processor.process_shap_batch(
    explainer=explainer,
    X=feature_data,
    batch_size=1000
)

# Create GPU-optimized plots
plots = gpu_processor.create_gpu_optimized_plots(
    shap_values=shap_values,
    X=feature_data,
    feature_names=feature_names,
    output_dir='shap_plots',
    asset='AAPL',
    model_type='xgboost',
    task='regression'
)
```

### Memory Monitoring
```python
from risk_pipeline.utils.gpu_shap_utils import get_gpu_memory_usage

# Check GPU memory usage
memory_info = get_gpu_memory_usage()
print(f"GPU Memory: {memory_info['allocated_mb']:.0f} MB allocated")
print(f"Free Memory: {memory_info['free_mb']:.0f} MB available")
print(f"Utilization: {memory_info['utilization']:.1%}")
```

## üéØ Model-Specific Optimizations

### XGBoost Models
- Uses `GPUTreeExplainer` for maximum speedup
- Automatically falls back to CPU `TreeExplainer` if GPU fails
- Optimized for tree-based SHAP computations

### LSTM/StockMixer Models
- GPU-accelerated tensor operations
- Batch processing for sequence data
- Memory-efficient background sampling

### All Models
- Automatic GPU memory management
- Configurable batch sizes
- Fallback to CPU processing if needed

## üìä Performance Tuning

### Batch Size Optimization
```python
# For large datasets (10k+ samples)
config.shap.batch_size_gpu = 2000

# For memory-constrained systems
config.shap.batch_size_gpu = 500
config.shap.gpu_memory_fraction = 0.6
```

### Memory Fraction Tuning
```python
# High memory usage (faster processing)
config.shap.gpu_memory_fraction = 0.9

# Conservative memory usage (more stable)
config.shap.gpu_memory_fraction = 0.6
```

## üîç Troubleshooting

### Common Issues

1. **CUDA Out of Memory**
   ```python
   # Reduce batch size and memory fraction
   config.shap.batch_size_gpu = 500
   config.shap.gpu_memory_fraction = 0.5
   ```

2. **GPU Not Detected**
   ```bash
   # Check CUDA installation
   nvidia-smi
   
   # Reinstall PyTorch with CUDA
   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
   ```

3. **SHAP GPU Installation Failed**
   ```bash
   # Install CPU-only version
   pip install shap==0.41.0
   
   # Or try alternative installation
   conda install -c conda-forge shap
   ```

### Debug Mode
```python
import logging
logging.getLogger('risk_pipeline').setLevel(logging.DEBUG)

# This will show detailed GPU memory usage and processing info
```

## üìà Expected Performance

| Dataset Size | CPU Time | GPU Time | Speedup |
|-------------|----------|----------|---------|
| 1,000 samples | 30s | 2s | 15x |
| 10,000 samples | 5min | 20s | 15x |
| 100,000 samples | 50min | 3min | 17x |

*Performance varies based on model complexity and GPU specifications.*

## üéâ Best Practices

1. **Start Small**: Begin with smaller batch sizes and increase gradually
2. **Monitor Memory**: Use `get_gpu_memory_usage()` to track memory usage
3. **Fallback Strategy**: Always have CPU fallback for production systems
4. **Batch Processing**: Use appropriate batch sizes for your GPU memory
5. **Memory Cleanup**: The system automatically clears GPU memory between operations

## üîó Additional Resources

- [SHAP GPU Documentation](https://shap.readthedocs.io/en/latest/example_notebooks/api_examples/explainers/GPUTree.html)
- [CUDA Installation Guide](https://docs.nvidia.com/cuda/cuda-installation-guide-linux/)
- [PyTorch CUDA Support](https://pytorch.org/get-started/locally/)

---

**Note**: GPU acceleration requires CUDA-compatible hardware and proper driver installation. The system will automatically fall back to CPU processing if GPU is not available.
